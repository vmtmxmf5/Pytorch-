{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e576417",
   "metadata": {},
   "source": [
    "### Merge Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def json_cat(path):\n",
    "    files = os.listdir(path=path[:-1])\n",
    "    \n",
    "    with open(path + files[0], encoding='utf-8') as f:\n",
    "        copy = json.load(f)\n",
    "    with open('testing.json', 'w') as f:\n",
    "        json.dump(copy, f, ensure_ascii=True)\n",
    "        \n",
    "    for file_idx in [185,1,2,3,4,5,400,401,550,551]:\n",
    "        file = files[file_idx]\n",
    "        with open(path+file, 'rb') as f:\n",
    "            data = f.read()\n",
    "        with open('testing.json', 'rb+') as f:\n",
    "            f.seek(-1, io.SEEK_END)\n",
    "            f.write(b', ' + data[1:])\n",
    "\n",
    "json_cat('C:/Users/CPB06GameN/글을쓰자/PyTorch-master/연습폴더/cleansing_json/')\n",
    "with open('testing.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db7544",
   "metadata": {},
   "source": [
    "### json Generator (ijson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "def json_loader(row_end=100000, block_size=10000, file='testing.json'):\n",
    "    src_text, tgt_text = [], []\n",
    "    src_lang, tgt_lang = [], []\n",
    "    \n",
    "    with open(file, 'rb') as f:\n",
    "        parse = ijson.parse(f)\n",
    "        for idx in range(1, 1+(18*row_end)):\n",
    "            item, key, cont = next(iter(parse))\n",
    "            if item == 'item.src_text':\n",
    "                src_text.append(cont)\n",
    "            if item == 'item.tgt_text':\n",
    "                tgt_text.append(cont)\n",
    "            if item == 'item.src_lang':\n",
    "                src_lang.append(cont)\n",
    "            if item == 'item.tgt_lang':\n",
    "                tgt_lang.append(cont)\n",
    "                \n",
    "            if (len(src_text) & len(tgt_text)) == block_size:\n",
    "                yield src_text, tgt_text, src_lang, tgt_lang\n",
    "                src_text, tgt_text, src_lang, tgt_lang = [], [], [], []\n",
    "\n",
    "        if (src_text or tgt_text):\n",
    "            yield src_text, tgt_text, src_lang, tgt_lang\n",
    "            \n",
    "# src_text, tgt_text = json_loader()\n",
    "data = json_loader(row_end=100000)\n",
    "print(json_loader)\n",
    "\n",
    "count = 0\n",
    "for epoch in range(2):\n",
    "    for src, tgt, src_lang, tgt_lang in data:\n",
    "        count += 1\n",
    "        print(src[0], tgt[0], '\\n', count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64513c",
   "metadata": {},
   "source": [
    "### Generator Data Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json_loader()\n",
    "\n",
    "def train_test_split(json_data):\n",
    "    train, validation = [], []\n",
    "    count = 0\n",
    "    for src_chunk, tgt_chunk, src_lang, tgt_lang in json_data:\n",
    "        for src, tgt, slang, tlang in zip(src_chunk, tgt_chunk, src_lang, tgt_lang):\n",
    "            sent = {'src_text':None, 'tgt_text':None, 'src_lang':None, 'tgt_lang':None}\n",
    "            if random.uniform(0, 1) <= 0.75:\n",
    "                sent['src_text'] = src\n",
    "                sent['tgt_text'] = tgt\n",
    "                sent['src_lang'] = slang\n",
    "                sent['tgt_lang'] = tlang\n",
    "                train.append(sent)\n",
    "            else:\n",
    "                sent['src_text'] = src\n",
    "                sent['tgt_text'] = tgt\n",
    "                sent['src_lang'] = slang\n",
    "                sent['tgt_lang'] = tlang\n",
    "                validation.append(sent)\n",
    "        count += 1\n",
    "        yield {'train':train, 'validation':validation}\n",
    "        train, validation = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c44a3",
   "metadata": {},
   "source": [
    "### Dynamic Batch Padding with Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def collate_fn_padd(batch):\n",
    "#     lengths = torch.tensor([t.shape[0] for dic in batch for t in dic.values()]).to(device)\n",
    "    batch = [t for dic in batch for t in dic.values()]\n",
    "    ids = torch.nn.utils.rnn.pad_sequence(batch[0::3])\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(batch[1::3])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(batch[2::3])\n",
    "    ids = ids.contiguous()\n",
    "    mask = mask.contiguous()\n",
    "    labels = labels.contiguous()\n",
    "#     return ids.t(), mask.t(), labels.t()\n",
    "    return {'input_ids':ids.t(), 'attention_mask':mask.t(), 'labels':labels.t()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad45db8",
   "metadata": {},
   "source": [
    "### Map-Style Chunk Generator + Tokenizer 실험 (실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import random\n",
    "\n",
    "checkpoint = 'facebook/m2m100_418M'\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(checkpoint, batched=True, truncation=True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_loader, tokenizer, key='train'):\n",
    "        super().__init__()\n",
    "        # loop 한 번 돌때마다 10,000개 chunk가 나오는 generator func\n",
    "        self.data = json_loader()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = {'train':[], 'validation':[]}\n",
    "        self.key = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100000\n",
    "    \n",
    "    def _encoding(self, chunk_idx):\n",
    "        count = 0\n",
    "        for src_chunk, tgt_chunk, src_lang, tgt_lang in self.data:\n",
    "            count += 1\n",
    "            if count == chunk_idx:\n",
    "                for src, tgt, src_lang, tgt_lang in zip(src_chunk, tgt_chunk, src_lang, tgt_lang):\n",
    "                    \n",
    "                    tokenized_sent = {'input_ids':None, 'attention_mask':None, 'labels':None}\n",
    "                    # 매번 lang을 바꾸면 시간이 오래 소요되지 않을까?\n",
    "                    # config 변경은 별로 안 걸릴 것 같은데??\n",
    "                    self.tokenizer.src_lang = src_lang\n",
    "                    self.tokenizer.tgt_lang = tgt_lang\n",
    "                    \n",
    "                    inputs = self.tokenizer(src, return_tensors='pt')\n",
    "                    with self.tokenizer.as_target_tokenizer():\n",
    "                        labels = tokenizer(tgt).input_ids\n",
    "                    \n",
    "                    if random.uniform(0, 1) <= 0.75:\n",
    "                        tokenized_sent['input_ids'] = inputs.input_ids.reshape(-1)\n",
    "                        tokenized_sent['attention_mask'] = inputs.attention_mask.reshape(-1)\n",
    "                        tokenized_sent['labels'] = torch.tensor(labels).reshape(-1)\n",
    "                        self.dataset['train'].append(tokenized_sent)\n",
    "                    else:\n",
    "                        tokenized_sent['input_ids'] = inputs.input_ids.reshape(-1)\n",
    "                        tokenized_sent['attention_mask'] = inputs.attention_mask.reshape(-1)\n",
    "                        tokenized_sent['labels'] = torch.tensor(labels).reshape(-1)\n",
    "                        self.dataset['validation'].append(tokenized_sent)\n",
    "        return self.dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index = index % 10000\n",
    "        chunk_idx = index // 10000\n",
    "        return self._encoding(chunk_idx+1)[self.key][index]\n",
    "\n",
    "\n",
    "test = CustomDataset(json_loader, tokenizer)\n",
    "\n",
    "test.__getitem__(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
