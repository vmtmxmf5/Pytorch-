{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기초조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze는 1인 axis를 모두 제거한다\n",
    "torch.FloatTensor([[[1, 2, 3]]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그냥 안에 쓰면 size로 인식된다\n",
    "torch.FloatTensor(2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt = torch.LongTensor([0,1,2,0])[..., None]\n",
    "oh = torch.zeros(4, 3)\n",
    "oh.scatter_(1, lt, 1) # lt를 idx 삼아서 1을 채운다\n",
    "# 부족하면 다시 lt의 idx를 0부터 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ByteTensor([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeeze\n",
    "torch.FloatTensor([[0],[1]])\n",
    "torch.FloatTensor([0,1]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [6]]) \n",
      " tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "#cat\n",
    "a = torch.arange(2)\n",
    "b = torch.arange(2,4)\n",
    "c = torch.arange(5,7)\n",
    "tmp1 = torch.cat([a.unsqueeze(1), b.unsqueeze(1), c.unsqueeze(1)])\n",
    "tmp2 = torch.cat([a.unsqueeze(0), b.unsqueeze(0), c.unsqueeze(0)])\n",
    "print(tmp1, '\\n', tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 20.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([4, 5])\n",
    "x.mul_(2).mul_(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피에 로그 가능도를 전개하면,\n",
    "# 교차 엔트로피는 최소화 = 음의 로그 가능도 최소화\n",
    "# 임을 수식적으로 증명할 수 있다\n",
    "\n",
    "# 교차 엔트로피를 mle관점에서 설명하자면\n",
    "# y분포(P)를 사용해서 y_hat(P_theta)의 엔트로피를 구하는 방식이다.\n",
    "# 교차 엔트로피의 최소화는 곧 로그 가능도의 최대화, mle이다.\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y_train)\n",
    "F.cross_entropy(z, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l2_norm 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = 0\n",
    "for param in model.parameters():\n",
    "    l2_reg += torch.norm(param)\n",
    "cost += l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7750)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.FloatTensor([4,5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchtext\n",
    "\n",
    "- TabularDataset.splits()  \n",
    "테이블형 데이터를 data.Field와 mapping 해서 불러오는 함수  \n",
    "\n",
    "- data.BucketIterator.split()  \n",
    "train과 valid를 한 번에 Bucket처럼 묶어서 iter하는 함수  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAB으로 구분된 text 데이터 불러오기\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self, train_fn, valid_fn, batch_size=64, device=-1, max_vocab=9e+10, min_freq=1, use_eos=False, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.label = data.Field(sequential=False,\n",
    "                               use_vocab=True,\n",
    "                               unk_token=None)\n",
    "        self.text = data.Field(use_vocab=True,\n",
    "                              batch_first=True,\n",
    "                              include_lengths=False,\n",
    "                              eos_token='<eos>' if use_eos else None)\n",
    "        train, valid = data.TabularDataset.splits(path='',\n",
    "                                                 train=train_fn,\n",
    "                                                 validation=valid_fn,\n",
    "                                                 format='tsv',\n",
    "                                                 fields=[('label', self.label),\n",
    "                                                        ('text', self.text)])\n",
    "        self.train_iter, self.valid_idter = data.BucketIterator.splits((train, valid),\n",
    "                                                                      batch_size=batch_size,\n",
    "                                                                      device='cuda:%d' % device if devcie >= 0 else 'cpu',\n",
    "                                                                      shuffle=shuffle,\n",
    "                                                                      sort_key=lambda x: len(x.text()),\n",
    "                                                                      sort_within_batch=True)\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-21e379bc0a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 레이블이 없이 문장으로만 된 데이터 불러오기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLMdataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# col이 튜플이나 list가 아닌 경우\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Dataset'"
     ]
    }
   ],
   "source": [
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "# 레이블이 없이 문장으로만 된 데이터 불러오기\n",
    "\n",
    "class LMdataset(data.Dataset):\n",
    "    def __init__(self, path, fields, max_length=None, **kwargs):\n",
    "        # col이 튜플이나 list가 아닌 경우\n",
    "        if not isinstance(fileds[0], (tuple, list)):\n",
    "            fields = [('text', fields[0])] # 튜플화 시켜서 리스트로 저장!\n",
    "        \n",
    "        examples = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if max_length <= len(line.split()):\n",
    "                    continue\n",
    "                if line != '':\n",
    "                    examples.append(data.Example.fromlist([line], fileds))\n",
    "        super(LMdataset, self).__init__(examples, fields, **kwargs)\n",
    "        \n",
    "\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self, train_fn, valid_fn, batch_size=64, device='cpu', max_vocab=9e+10, max_length=255,\n",
    "                fix_length=None, use_bos=True, use_eos=True, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.text = data.Field(sequence=True,\n",
    "                              use_vocab=True,\n",
    "                              batch_first=True,\n",
    "                              include_length=True,\n",
    "                              fix_length=fix_length,\n",
    "                              init_token= '<BOS>' if use_bos else None,\n",
    "                              eos_token='<EOS>' if use_eos else None)\n",
    "        train = LMdataset(path=train_fn,\n",
    "                         fields=[('text', self.text)],\n",
    "                         max_length=max_length)\n",
    "        valid = LMdataset(path=valid_fn,\n",
    "                         fields=[('text', self.text)],\n",
    "                         max_length=max_length)\n",
    "        \n",
    "        self.train_iter = data.BucketIterator(train,\n",
    "                                             batch_size=batch_size,\n",
    "                                             # format == %\n",
    "                                             device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                                             shuffle=shuffle,\n",
    "                                             sort_key=lambda x: -len(x.text),\n",
    "                                             sort_within_batch=True)\n",
    "        self.valid_iter = data.BucketIterator(valid,\n",
    "                                             batch_size=batch_size,\n",
    "                                             device='cuda:%d' % device if device >=0 else 'cpu',\n",
    "                                             shuffle=False,\n",
    "                                             sort_key=lambda x: -len(x.text),\n",
    "                                             sort_within_batch=True)\n",
    "        \n",
    "        self.text.build_vocab(train, max_size=max_vocab)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-245fa9df08bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mPAD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBOS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEOS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msort_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Dataset'"
     ]
    }
   ],
   "source": [
    "# 텍스트로만 된 데이터 2개 불러오기\n",
    "\n",
    "PAD, BOS, EOS = 1, 2, 3\n",
    "\n",
    "class TranslationDataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
    "    \n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwargs):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
    "        if not path.endswith('.'):\n",
    "            path += '.'\n",
    "        \n",
    "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
    "        \n",
    "        examples = []\n",
    "        with open(src_path, encoding='utf-8') as src_file, open(trg_path, encoding='utf-8') as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if max_length and max_length < max(len(src_line.split(), trg_line.split())):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples.append(data.Example.fromlist([src_line, trg_line], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "    \n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self, train_fn=None, valid_fn=None, exts=None, batch_size=64, device='cpu', max_vocab=9e+10, \n",
    "                 max_length=255, fix_length=None, use_bos=True, use_eos=True, shuffle=True, dsl=False):\n",
    "        super().__init__()\n",
    "        self.src = data.Field(sequential=True,\n",
    "                             use_vocab=True,\n",
    "                             batch_first=True,\n",
    "                             include_length=True,\n",
    "                             fix_length=fix_length,\n",
    "                             init_token='<BOS>' if dsl else None,\n",
    "                             eos_token='<EOS>' if dsl else None)\n",
    "        self.tgt = data.Field(sequential=True,\n",
    "                             use_vocab=True,\n",
    "                             batch_first=True,\n",
    "                             include_lengths=True,\n",
    "                             fix_length=fix_lenth,\n",
    "                             init_token='<BOS>' if use_bos else None,\n",
    "                             eos_token='<EOS>' if use_eos else None)\n",
    "        \n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            train = TranslationDataset(path=train_fn,\n",
    "                                      exts=exts,\n",
    "                                      fields=[('src', self.src), ('tgt', self.tgt)],\n",
    "                                      max_length=max_length)\n",
    "            valid = TranslationDataset(path=valid_fn,\n",
    "                                      exts=exts,\n",
    "                                      fields=[('src', self.src), ('tgt', self.tgt)],\n",
    "                                      max_length=max_length)\n",
    "            self.train_iter = data.BucketIterator(train,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                                                 shuffle=shuffle,\n",
    "                                                 sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                                                 sort_within_batch=True)\n",
    "            self.valid_iter = data.BucketIterator(valid,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "                                                 shuffle=False,\n",
    "                                                 sort_key=lambda x: len(x.tgt) + (max_length * len(x.src)),\n",
    "                                                 sort_within_batch=True)\n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "            \n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.src.vocab = src_vocab\n",
    "        self.tgt.vocab = tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.839px",
    "left": "647.42px",
    "right": "20px",
    "top": "23px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
