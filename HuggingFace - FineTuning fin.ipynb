{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbafb91",
   "metadata": {},
   "source": [
    "## HF A to Z : Trainer 사용\n",
    "\n",
    "#### tokenize_function\n",
    "> Map-style 사용을 위한 함수  \n",
    "- tokenizer 실행시 메모리 관리를 위해 **함수**를 만들고 **map**을 사용한다\n",
    "  \n",
    "#### DataCollatorWithPadding  \n",
    "  \n",
    ">동적 패딩방식\n",
    "- **배치마다 가장 큰 seq_len를 기준**으로, 배치별로 서로 다르게 패딩을 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02341507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4065"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "267c252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b13a32b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c4a4faf6694466967fa292ab85ea40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2751' max='2751' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2751/2751 08:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.469604</td>\n",
       "      <td>0.781863</td>\n",
       "      <td>0.833645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.720630</td>\n",
       "      <td>0.821078</td>\n",
       "      <td>0.873043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.739696</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.889643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2751, training_loss=0.5209580161102639, metrics={'train_runtime': 488.3194, 'train_samples_per_second': 22.534, 'train_steps_per_second': 5.634, 'total_flos': 377309475606720.0, 'train_loss': 0.5209580161102639, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# map-style을 활용하여 **메모리를 절약**하기 위해 func을 만든다\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric('glue', 'mrpc')\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # 동적 패딩\n",
    "training_args = TrainingArguments('test-trainer',\n",
    "                                  evaluation_strategy='epoch', # 저장 경로 test-trainer\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4) \n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "raw_dataset = load_dataset('glue', 'mrpc')\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "trainer = Trainer(model,\n",
    "                 training_args,\n",
    "                 train_dataset=tokenized_dataset['train'],\n",
    "                 eval_dataset=tokenized_dataset['validation'],\n",
    "                 data_collator=data_collator,\n",
    "                 tokenizer=tokenizer,\n",
    "                 compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfc2b1",
   "metadata": {},
   "source": [
    "## HF A to Z : Pytorch 사용\n",
    "\n",
    "- metric 함수를 따로 만들지 않아도 된다  \n",
    "  \n",
    "  \n",
    "- dataset의 필요한 col만 추출해서 직접 DataLoader에 태워야 한다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a883d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb7c8ab54e54844bb8c250737cf4913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True)\n",
    "\n",
    "training_args = TrainingArguments('del-dir', evaluation_strategy='epoch')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model.to(device) # model-gpu\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "raw_dataset = load_dataset('glue', 'mrpc')\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# col : sentence1  sentence2  idx  label  attention_mask  input_ids  token_type_ids\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=4, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=4, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "504730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "num_eval_steps = num_epochs * len(eval_dataloader)\n",
    "lr_scheduler = get_scheduler('linear',\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=0,\n",
    "                            num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e91f685",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d734a9560d5b428ea7fd7d4f8d1b26a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2751 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09baf67a512c453d9b503f052efef486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cpb06gamen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\datasets\\formatting\\torch_formatter.py:50: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data_struct.dtype == np.object:  # pytorch tensors cannot be instantied from an array of objects\n",
      "c:\\users\\cpb06gamen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\datasets\\formatting\\torch_formatter.py:50: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data_struct.dtype == np.object:  # pytorch tensors cannot be instantied from an array of objects\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "metrics = {'train':[], 'validation':[]}\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k:v.to(device) for k, v in batch.items()} # data-gpu\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_loss /= num_training_steps\n",
    "    metrics['train'] = train_loss\n",
    "\n",
    "\n",
    "progress_bar = tqdm(range(num_eval_steps))\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "model.eval()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k:v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch)\n",
    "        logits = output.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=preds, references=batch['labels'])\n",
    "        progress_bar.update(1)\n",
    "    metric.compute()\n",
    "    metrics['validation'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3e9a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = metrics['validation']\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7929a83",
   "metadata": {},
   "source": [
    "## HF A to Z : Accelerator + Pytorch 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transfomrers import AutoModelForSequenceClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenize(example['sentence1'], example['sentence2'], truncation=True)\n",
    "\n",
    "training_args = TrainingArguments('del-dir', evalutation_strategy='epoch')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "raw_dataset = load_dataset('glue', 'mrpc')\n",
    "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets = tokenzied_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=4, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=4, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import AdamW, get_scheduler\n",
    "from accelerate import Accelerator\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = epoch * len(train_dataloader)\n",
    "num_eval_steps = epoch * len(eval_dataloader)\n",
    "lr_scheduler = get_scheduler('linear',\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=0,\n",
    "                            num_training_steps=num_training_steps)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(train_dataloader,\n",
    "                                                                         eval_dataloader,\n",
    "                                                                         model,\n",
    "                                                                         optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c5d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.857px",
    "left": "878.531px",
    "right": "20px",
    "top": "9px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
