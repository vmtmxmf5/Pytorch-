{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f4d592",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2d30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('C:/dd/petition_sampled.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581c544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_sp_wh(text):\n",
    "    text = re.sub(r'([\\t\\f\\r\\v\\n])|([^ ㄱ-ㅎ가-힣0-9]+)', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "df.title = df.title.apply(remove_sp_wh)\n",
    "df.content = df.content.apply(remove_sp_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ddbbff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "df['title_token'] = df.title.apply(okt.morphs)\n",
    "df['content_token'] = df.content.apply(okt.nouns)\n",
    "df['token_final'] = df.title_token + df.content_token\n",
    "df['count'] = df['votes'].replace({',':''}, regex=True).apply(lambda x: int(x))\n",
    "df['lable'] = df['count'].apply(lambda x: 'yes' if x>=1000 else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08bb772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['token_final', 'lable']].copy()\n",
    "df.rename(columns={'lable':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ec8ee",
   "metadata": {},
   "source": [
    "## 임베딩 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7710b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cpb06gamen\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "embedding_vector = Word2Vec(df['token_final'],\n",
    "                           sg=1,\n",
    "                           vector_size=100,\n",
    "                           window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434a5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('평창', 0.8491191267967224), ('평창올림픽', 0.8447761535644531), ('동계올림픽', 0.8439947962760925), ('평창동계올림픽', 0.8237838745117188), ('월드컵', 0.8197010159492493), ('출전', 0.8047665953636169), ('참가', 0.7970430254936218), ('패럴림픽', 0.7910754084587097), ('동계', 0.7835447192192078), ('선수단', 0.773549497127533)]\n"
     ]
    }
   ],
   "source": [
    "model_result = embedding_vector.wv.most_similar('올림픽')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed24f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector.wv.save_word2vec_format('petition_token_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format('petition_token_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a470e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field\n",
    "\n",
    "rng = RandomState()\n",
    "train_data = df.sample(frac=.8, random_state=rng)\n",
    "test_data = df.loc[~df.index.isin(train_data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a04061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub(r'[\\[\\]\\']', '', str(text))\n",
    "    text = text.split(', ')\n",
    "    return text\n",
    "\n",
    "# batch_first없으면 train함수에서 torch.transpose해줘야 함\n",
    "TEXT = Field(tokenize=tokenizer, batch_first=True)\n",
    "LABEL = Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ef72950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('C:/dd/train_petition.csv', index=False)\n",
    "test_data.to_csv('C:/dd/valid_petition.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b832f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "train, valid = TabularDataset.splits(\n",
    "    path = 'C:/dd/',\n",
    "    train = 'train_petition.csv',\n",
    "    validation = 'valid_petition.csv',\n",
    "    format = 'csv',\n",
    "    fields = [('text', TEXT), ('label', LABEL)],\n",
    "    skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2df7b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "vectors = Vectors(name='petition_token_w2v')\n",
    "\n",
    "TEXT.build_vocab(train, vectors=vectors, min_freq=1, max_size=None)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    datasets = (train, valid),\n",
    "    batch_size = 8,\n",
    "    device = device,\n",
    "    sort = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53e70a",
   "metadata": {},
   "source": [
    "## Text CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80c98fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim, c_out, kernel_wins, num_class):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(len(vocab), emb_dim)\n",
    "        # pre-trained vocab을 가져오기\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "        \n",
    "        # Conv2d(c_in, c_out, filter_size=(kernel_size, emb_dim))\n",
    "        # self.convs의 filter size : [3*100, 4*100, 5*100]\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, c_out, (w, emb_dim)) for w in kernel_wins])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        # 종류별 feature map의 개수 만큼 unit과 연결된다\n",
    "        self.fc = nn.Linear(len(kernel_wins)*c_out, num_class)\n",
    "        # output size = # of units\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : (timesteps)\n",
    "        # emb_lookup : (timestep, emb_dim)\n",
    "        emb = self.embed(x)\n",
    "        \n",
    "        # cnn에 넣으려면 이미지 처럼 2차원이어야 한다\n",
    "        # (Batch, vocab_size, emb_dim) ==> (Batch, 1, vocab_size, emb_dim) : 채널 추가\n",
    "        emb = emb.unsqueeze(1)\n",
    "        \n",
    "        # conv output shape : [c_out, H=kernel_size*emb_dim, W=1]\n",
    "        # Width가 1인 이유? 필터 가로 크기와 input 가로 길이가 같으니까\n",
    "        con = [self.relu(conv(emb)) for conv in self.convs] \n",
    "        \n",
    "        # max_pool1d(input=(c_out, kernel_size*emb_dim), kernel_size=H)\n",
    "        pool = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con]\n",
    "        # pool output =  [c_out * 1, c_out * 1, c_out * 1] \n",
    "        # 필터 크기와 conv output의 크기가 동일하니까 채널과 1만 남는거 ㅇ\n",
    "        \n",
    "        # pool 1개 output shape : (Batch, c_out, 1)\n",
    "        fc = torch.cat(pool, dim=1) # 필터 크기별로 cnn 돌린거 종합\n",
    "        fc = fc.squeeze(-1) # 10*1 을 10기준으로 concat한 뒤, 1 빈 리스트 제거\n",
    "        fc = self.dropout(fc)\n",
    "        \n",
    "        logit = self.fc(fc)\n",
    "        return logit     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfd8e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_iter, optimizer):\n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0, 0.0\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        text, target = batch.text, batch.label\n",
    "        # 이미 batch_first를 사용했기 때문에 필요없음\n",
    "#         text = torch.transpose(text, 0, 1) # 1배치 1로우\n",
    "        target.data.sub_(1) # target - 1\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        result = torch.max(logit, 1)[1] # 가장 큰 확률, 실제 값(0은 argmax임)\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "        \n",
    "    train_loss /= len(train_iter.dataset) # == train_loss/Batch_size\n",
    "    accuracy = 100.0 * corrects / len(train_iter.dataset)\n",
    "    \n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "111a9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, eval_iter):\n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0, 0.0\n",
    "    \n",
    "    for batch in eval_iter:\n",
    "        text, target = batch.text, batch.label\n",
    "#         text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        \n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit, 1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "        \n",
    "    test_loss /= len(eval_iter.dataset)\n",
    "    accuracy = 100.0*corrects / len(eval_iter.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67f34d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 \t Loss: 0.009061651507467162 \t accuracy: 98.818%\n",
      "valid epoch: 1 \t Loss: 0.005662299163206732 \t accuracy: 99.281%\n",
      "model saved at 99.28077697753906 accuracy\n",
      "train epoch: 2 \t Loss: 0.007278435231444717 \t accuracy: 99.011%\n",
      "valid epoch: 2 \t Loss: 0.0053660755365813875 \t accuracy: 99.281%\n",
      "train epoch: 3 \t Loss: 0.00608348290886248 \t accuracy: 98.984%\n",
      "valid epoch: 3 \t Loss: 0.005500715491229681 \t accuracy: 99.281%\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN(vocab, 100, 10, [3, 4, 5], 2).to(device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "best_test_acc = -1\n",
    "\n",
    "for epoch in range(1, 3+1):\n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer)\n",
    "    print('train epoch: {} \\t Loss: {} \\t accuracy: {:.3f}%'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, device, valid_iter)\n",
    "    print('valid epoch: {} \\t Loss: {} \\t accuracy: {:.3f}%'.format(epoch, val_loss, val_acc))\n",
    "    \n",
    "    if val_acc > best_test_acc:\n",
    "        best_test_acc = val_acc\n",
    "        \n",
    "        print('model saved at {} accuracy'.format(best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c548e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce2dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "237.844px",
    "left": "1410.12px",
    "right": "20px",
    "top": "14px",
    "width": "464px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
