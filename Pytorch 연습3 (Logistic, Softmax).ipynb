{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6dc683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabc882",
   "metadata": {},
   "source": [
    "## Logistic 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48074ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471824645996\n",
      "0.34497418999671936\n",
      "0.2405989021062851\n",
      "0.17620913684368134\n",
      "0.1346660703420639\n",
      "0.1063789650797844\n",
      "0.086177296936512\n",
      "0.07117627561092377\n",
      "0.05968214571475983\n",
      "0.05064987763762474\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "W = torch.zeros((X_train.shape[1], 1), requires_grad=True)\n",
    "b = torch.zeros(y_train.shape[1], requires_grad=True)\n",
    "optm = optim.Adam([W, b], lr=1e-2)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    h = 1/(1+torch.exp(-(X_train.matmul(W) + b)))\n",
    "    # torch.sigmoid(X_train.matmul(W) + b)\n",
    "    cost = -torch.mean(y_train * torch.log(h) + (1-y_train) * torch.log(1-h))\n",
    "    # F.binary_cross_entropy(h, y_train)\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    cost.backward()\n",
    "    optm.step()\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff5c18",
   "metadata": {},
   "source": [
    "## Logistic nn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492ef96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, cost : 1.378485, acc: 0.5 vs 0.5\n",
      "epoch : 1, cost : 1.087292, acc: 0.5 vs 0.5\n",
      "epoch : 1, cost : 0.879674, acc: 0.5 vs 0.5\n",
      "epoch : 251, cost : 0.282517, acc: 1.0 vs 1.0\n",
      "epoch : 251, cost : 0.858365, acc: 0.0 vs 0.0\n",
      "epoch : 251, cost : 0.631919, acc: 0.5 vs 0.5\n",
      "epoch : 501, cost : 0.431983, acc: 1.0 vs 1.0\n",
      "epoch : 501, cost : 0.650443, acc: 0.5 vs 0.5\n",
      "epoch : 501, cost : 0.540437, acc: 0.5 vs 0.5\n",
      "epoch : 751, cost : 0.731196, acc: 0.5 vs 0.5\n",
      "epoch : 751, cost : 0.481443, acc: 1.0 vs 1.0\n",
      "epoch : 751, cost : 0.314292, acc: 1.0 vs 1.0\n",
      "epoch : 1001, cost : 0.631775, acc: 0.5 vs 0.5\n",
      "epoch : 1001, cost : 0.444161, acc: 1.0 vs 1.0\n",
      "epoch : 1001, cost : 0.376401, acc: 1.0 vs 1.0\n",
      "epoch : 1251, cost : 0.334190, acc: 1.0 vs 1.0\n",
      "epoch : 1251, cost : 0.348010, acc: 1.0 vs 1.0\n",
      "epoch : 1251, cost : 0.714050, acc: 0.5 vs 0.5\n",
      "epoch : 1501, cost : 0.692780, acc: 0.5 vs 0.5\n",
      "epoch : 1501, cost : 0.359134, acc: 1.0 vs 1.0\n",
      "epoch : 1501, cost : 0.301841, acc: 1.0 vs 1.0\n",
      "epoch : 1751, cost : 0.354100, acc: 1.0 vs 1.0\n",
      "epoch : 1751, cost : 0.388729, acc: 1.0 vs 1.0\n",
      "epoch : 1751, cost : 0.574069, acc: 0.5 vs 0.5\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "X_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "ds = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optm = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    for batch_idx, sample in enumerate(ds):\n",
    "        X_train, y_train = sample\n",
    "        pred = model(X_train)\n",
    "        loss = F.binary_cross_entropy(pred, y_train)\n",
    "\n",
    "        optm.zero_grad()\n",
    "        loss.backward()\n",
    "        optm.step()\n",
    "        \n",
    "        y_pred = torch.where(pred >= 0.5, torch.FloatTensor([1]), torch.FloatTensor([0]))\n",
    "        \n",
    "        # 배치마다 정확도 계산\n",
    "        acc = (y_train == y_pred).sum().item() / y_train.size(0)\n",
    "        acc2 = (y_train == y_pred).to(torch.float).mean()\n",
    "        if epoch % 250 == 0:\n",
    "            # 수치가 이상한 이유? batch_size가 너무 작아서\n",
    "            print('epoch : {}, cost : {:.6f}, acc: {} vs {}'.format(epoch+1, loss.item(), acc, acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a9974",
   "metadata": {},
   "source": [
    "## Logistic class 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb68bf6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Bool for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c84cdc6e372c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m250\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Bool for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "X_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "class logistic_reg(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        z = self.linear(x)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "model = logistic_reg()\n",
    "optm = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    h = model(X_train)\n",
    "    loss = F.binary_cross_entropy(h, y_train)\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    loss.backward()\n",
    "    optm.step()\n",
    "    \n",
    "    if epoch % 250 == 0:\n",
    "        y_pred = (h > torch.FloatTensor([0.5]))\n",
    "        acc = (y_train == y_pred).to(torch.float).mean().item()\n",
    "        print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15228a8",
   "metadata": {},
   "source": [
    "## Softmax 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed6b1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상한, size\n",
    "# long == 64bit\n",
    "y = torch.randint(5, (4,)).long()\n",
    "y_oh = torch.zeros((4,5))\n",
    "y_oh.scatter_(1, y.unsqueeze(1), 1) #dim, idx, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ccbcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_data = [2, 2, 2, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a804d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "y_oh = torch.zeros(y_train.shape[0], len(y_train.unique()))\n",
    "y_oh.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "\n",
    "# 클래스 3개\n",
    "W = torch.zeros((X_train.shape[1], len(y_train.unique())), requires_grad=True)\n",
    "b = torch.zeros((len(y_train.unique()),), requires_grad=True)\n",
    "\n",
    "optm = torch.optim.Adam([W, b], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "20539e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986123085021973\n",
      "0.9155282974243164\n",
      "0.8263546228408813\n",
      "0.7666869759559631\n",
      "0.7213972210884094\n",
      "0.683803141117096\n",
      "0.6512508392333984\n",
      "0.6225917935371399\n",
      "0.5971412658691406\n",
      "0.5743632912635803\n",
      "0.5538042783737183\n",
      "0.5350828766822815\n",
      "0.5178844928741455\n",
      "0.5019524693489075\n",
      "0.4870786964893341\n",
      "0.47309410572052\n",
      "0.45986199378967285\n",
      "0.4472711682319641\n",
      "0.43523111939430237\n",
      "0.42366793751716614\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    # OH\n",
    "    y_hat = F.softmax(X_train.matmul(W) + b, dim=1)\n",
    "    cost = (y_oh * -torch.log(y_hat)).sum(dim=1).mean()\n",
    "    # sum(dim=1) : 원핫과 계산해서 0이 된 열들을 하나로 모아주는 작업\n",
    "    \n",
    "    # ~OH\n",
    "    z = X_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    cost.backward()\n",
    "    optm.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151ea9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4635, 0.6038, 0.5899, 0.9874, 0.0617, 0.9267, 0.7585, 0.0285, 0.3073,\n",
       "        0.6737], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(10).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d2bdb",
   "metadata": {},
   "source": [
    "## Softmax nn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d715fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.035637855529785\n",
      "0.8860167264938354\n",
      "0.7366489768028259\n",
      "0.6534460783004761\n",
      "0.6000434756278992\n",
      "0.5607463121414185\n",
      "0.5292974710464478\n",
      "0.5028305649757385\n",
      "0.4797782003879547\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "X_train = torch.FloatTensor(X_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "\n",
    "model = nn.Linear(X_train.shape[1], len(y_train.unique()))\n",
    "optm = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(2001):\n",
    "    z = model(X_train)\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    cost.backward()\n",
    "    optm.step()\n",
    "    \n",
    "    if epoch % 250 == 0:\n",
    "        print(cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e1271",
   "metadata": {},
   "source": [
    "## Softmax class 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48f56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "X_train = torch.FloatTensor(X_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "\n",
    "class softmax(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(X_train.shape[1], len(y_train.unique()))\n",
    "    def forward(self, x):\n",
    "        z = self.linear(x)\n",
    "        return F.log_softmax(z, dim=1)\n",
    "\n",
    "model = softmax()\n",
    "optm = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06794e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.035637855529785\n",
      "1.0145037174224854\n",
      "0.9468469619750977\n",
      "0.9018084406852722\n",
      "0.8685850501060486\n",
      "0.8422440886497498\n",
      "0.8202191591262817\n",
      "0.8010996580123901\n",
      "0.7840710878372192\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2001):\n",
    "    a = model(X_train)\n",
    "    cost = F.nll_loss(a, y_train)\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    cost.backward()\n",
    "    optm.step()\n",
    "    \n",
    "    if epoch % 250 == 0:\n",
    "        print(cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a7e0d",
   "metadata": {},
   "source": [
    "## Mnist 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a9cdd41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dsets' from 'torchvision.datasets' (/home/jaehoon/anaconda3/envs/torch_env/lib/python3.7/site-packages/torchvision/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d682c7dc9e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dsets' from 'torchvision.datasets' (/home/jaehoon/anaconda3/envs/torch_env/lib/python3.7/site-packages/torchvision/datasets/__init__.py)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.83px",
    "left": "674.429px",
    "right": "20px",
    "top": "82px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
