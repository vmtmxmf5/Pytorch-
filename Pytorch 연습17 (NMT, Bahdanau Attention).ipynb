{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda4c94b",
   "metadata": {},
   "source": [
    "### 바다나우 어텐션 vs 류옹 어텐션  \n",
    "| Dot Attention | Bahdanau Attention |\n",
    "|--|--|\n",
    "| $score(s_{t},\\ h_{i}) = s^{T}_{t}h_{i}$ | $score(s_{t-1},\\ H) = W_{a}^{T}\\ tanh(W_{b}s_{t-1}+W_{c}H)$ |\n",
    "| RNN cell 초기값을 encoder의 last hidden state로 설정한다 | 1) **context vector를 input과 concat**해서 RNN cell의 입력으로 보낸다 |\n",
    "| 1) context vector를 s_t와 concat한 뒤 출력층 계산 | 2) 그렇게 나온 s_t와 context vector를 다시 concat해서 출력층 계산 |\n",
    "\n",
    "\\*) context vector == attention value == softmax(attention score) * Values  \n",
    "\\**) s_t == decoder output == decoder hidden state  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f767276",
   "metadata": {},
   "source": [
    "1. decoder att은 무엇과 concat 하는가?  \n",
    " v[s1 ; att_0] => Dense(len(vocab))  \n",
    "\n",
    "2. code 상에 query와 key는 어떻게 표현되는가?(하이퍼 파라미터, 인풋)  \n",
    "encoder의 hidden states가 K, V다  \n",
    "쿼리는??  decoder의 cell state s_t\n",
    "\n",
    "3. att에서 bidirection은 default인가  \n",
    "ㄴㄴ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de,\n",
    "           init_token='<SOS>',\n",
    "           eos_token='<EOS>',\n",
    "           lower=True)\n",
    "TGT = Field(tokenize=tokenize_en,\n",
    "           init_token='<SOS>',\n",
    "           eos_token='<EOS>',\n",
    "           lower=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                   fields=(SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TGT.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                         batch_size = batch_size,\n",
    "                                                         device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fe96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim) # input_dim = len(vocab)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        # 왜 time-step은 하이퍼파라미터가 아닌가? \n",
    "        # 배치 사이즈와 time-step은 input data에 shape이기 때문!\n",
    "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim) # bidirection이니까 뉴런도 두 배로 나오겠지\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    def forward(self, src)\n",
    "        # src = (time-step, batch size)\n",
    "        emb = self.dropout(self.embedding(src))\n",
    "        # emb = (time-step, batch size, emb dim)\n",
    "        output, last_hidden_state = self.rnn(emb)\n",
    "        # output = (time-step, batch size, hid dim * num direction)\n",
    "        # last hidden state = (num direction * layers, batch size, hid dim)\n",
    "        # [(forward, backward), (forward, backward), ...] \n",
    "        hidden = torch.tanh(self.fc(torch.cat((last_hidden_state[-2::], last_hidden_state[-1::]), dim=1)))\n",
    "        return output, hidden # 레이어의 마지막 출력층, context vectors\n",
    "    \n",
    "        \n",
    "# 특정 query와 key 값을 concat한 다음 linear 돌려서, query shape으로 만듬\n",
    "# 그 query shape을 다시 1로 압축함\n",
    "class Attention(nn.Module):\n",
    "    # 원래 decoder의 출력 개수는 뉴런 개수(s_t)임\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        # query 뉴런 개수 + key 뉴런 개수 * 2 (bidirection 이니까)\n",
    "        self.attn = nn.Linear(dec_hid_dim + (enc_hid_dim * 2), dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # encoder_output : (time-step, batch_size, enc hid dim * 2) == keys\n",
    "        # hidden : (batch_size, dec hid dim) == s_{t-1} == query\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        time_steps = encoder_outputs.shape[0]\n",
    "        \n",
    "        # encoder의 output에 time step 개수 만큼 context vector를 반복\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, time_steps, 1) # (batch_size, time-step, dec hid dim)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # (batch_size, time-step, enc hid dim * 2)\n",
    "        \n",
    "        # 원래는 tf.nn.tanh(self.W1(keys) + self.W2(query)) 인데 걍 concat으로 한 방에 계산 ㄱㄱ\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # : (batch_size, time-step, dec hid dim)\n",
    "        \n",
    "        # attention score: (batch_size, time-step)\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1) # att weights\n",
    "        # 여기다가 value랑 곱하면 context vector\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim  # len(vocab)\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, attention)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        # inputs : [batch_size]\n",
    "        # hidden : [batch_size, dec hid dim] == decoder output == query == s_{t-1}\n",
    "        # encoder_outputs : [time-step, batch_size, enc hid dim * 2] == key == value\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0) # [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        attention_weights = attention_weights.unsqueeze(1) # [batch_size, 1, time-step] decoder time-step 1추가\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # axis 0 1 2 ==> axis 1 0 2 변경\n",
    "        \n",
    "        # context vector : [batch_size, 1, enc_hid_dim * 2]\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs) # batch matrix multiplication\n",
    "        weighted = weighted.permute(1, 0, 2) \n",
    "        \n",
    "        # embedding vector + context vector\n",
    "        rnn_input = torch.cat([embedded, weighted], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0)) # 이전 시점의 hidden을 넣는다\n",
    "        # output : [time-step, batch_size, dec hid dim * n direction(=1)]\n",
    "        # hidden : [n layers * n direction, batch_size, dec hid dim]\n",
    "        assert (output == hidden).all() # [1, batch_size, dec hid dim]\n",
    "        \n",
    "        embedded = embedded.squeeze(0) # [batch_size, emb dim]\n",
    "        weighted = weighted.squeeze(0) # [batch_size, enc hid dim * 2]\n",
    "        output = output.squeeze(0) # [batch_size, dec hid dim]\n",
    "        \n",
    "        # y_hat + context vector + embed (이 구현에서만 embed는 추가된거)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # : [batch_size, vocab_size]\n",
    "        return prediction, hidden.squeeze(0) # 예측값, s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b0e3317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4.],\n",
      "         [5.],\n",
      "         [6.]],\n",
      "\n",
      "        [[1.],\n",
      "         [2.],\n",
      "         [3.]]]) torch.Size([2, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[4.],\n",
       "          [1.]],\n",
       " \n",
       "         [[5.],\n",
       "          [2.]],\n",
       " \n",
       "         [[6.],\n",
       "          [3.]]]),\n",
       " torch.Size([3, 2, 1]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tmp = torch.FloatTensor([[[4],[5],[6]],[[1],[2],[3]]])\n",
    "print(tmp, tmp.size())\n",
    "\n",
    "# axis 0 1 2 ==> axis 1 0 2 변경\n",
    "temp = tmp.permute(1, 0, 2)\n",
    "temp, temp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e497285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.device = device()\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.75):\n",
    "        # src : [encoder time-step, batch_size]\n",
    "        # trg : [decoder time-step, batch_size]\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0] # decoder time-step size\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        inputs = trg[0,:]  # sos 토큰\n",
    "        # encoder hidden states, 마지막 셀 hidden state (forward, backward 각각)\n",
    "        encoder_outputs, last_hidden = self.encoder(src)\n",
    "        \n",
    "        # outputs : [decoder time-step, batch_size, decoder vocab size]\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        for token in range(1, trg_len):\n",
    "            output, hidden = self.decoder(inputs, last_hidden, encoder_outputs) # sos, s_{t-1} query, key=value\n",
    "            outputs[token] = output # decoder의 prediction\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            inputs = trg[token] if teacher_force else top1\n",
    "        return outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a19602f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16781948911776257"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "enc_emb_dim, dec_emb_dim = 256, 256\n",
    "enc_hid_dim, dec_hid_dim = 512, 512\n",
    "enc_dropout, dec_dropout = 0.4, 0.4\n",
    "\n",
    "attn = Attention(enc_hid_dim, dec_hid_dim)\n",
    "enc = Encoder(input_dim, enc_emb_dim, enc_hid_dim, dec_hid_dim, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, enc_hid_dim, dec_hid_dim, dec_dropout, attn)\n",
    "model = Seq2seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    cnt = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return print(f'The model has {cnt:,} trainable parameters')\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d6098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch.src, batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src, trg)  # [trg time-step, batch_size, trg vocab_size]\n",
    "        \n",
    "        output_dim = pred.shape[-1]\n",
    "        pred = pred[1, :].view(-1, output_dim) # [(trg - 1) * batch_size, trg vocab size]\n",
    "        trg = trg[1, :].view(-1) # [(trg - 1) * batch_size]\n",
    "        loss = criterion(pred, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def eval_model(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, trg = batch.src, batch.trg\n",
    "            \n",
    "            pred = model(src, trg, 0) # teacher forcing 종료\n",
    "            \n",
    "            output_dim = pred.shape[-1]\n",
    "            pred = pred[1, :].view(-1, output_dim)\n",
    "            trg = trg[1, :].view(-1)\n",
    "            loss = criterion(pred, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "max_epochs = 10\n",
    "clip = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_model(model, train_iter, optimizer, criterion, clip)\n",
    "    valid_loss = eval_model(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    print('Epoch: {:02}, time: {}m{}s train loss: {:.3f}, valid loss: {:.3f}'.format(epoch+1, epoch_mins, epoch_secs, train_loss, valid_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = eval_model(model, test_iter, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
