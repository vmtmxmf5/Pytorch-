{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "logging.disable('WARNING')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec05be2",
   "metadata": {},
   "source": [
    "- **pd.read_json 사용 가능하도록 사전 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36411972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('testing.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    data = data[:1000]\n",
    "with open('test2.json', 'w') as f:\n",
    "    json.dump(data, f, ensure_ascii=True)\n",
    "data = pd.read_json('test2.json')\n",
    "data.to_json('test2.json', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29626c",
   "metadata": {},
   "source": [
    "## Customizing Iterable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "checkpoint = 'facebook/m2m100_418M'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(checkpoint, batched=True)\n",
    "\n",
    "class MyDataset(IterableDataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.path = path\n",
    "    def __iter__(self):\n",
    "        chunksize = 10\n",
    "        iter_json = pd.read_json(self.path, lines=True, chunksize=chunksize)\n",
    "        for chunk in iter_json:\n",
    "            for idx in range(chunksize):\n",
    "                tokenized_data = {'input_ids':None, 'attention_mask':None, 'labels':None}\n",
    "                length = []\n",
    "                idx = idx + chunk.index[0]\n",
    "                worker = torch.utils.data.get_worker_info()\n",
    "                worker_id = worker.id if worker is not None else -1\n",
    "                \n",
    "                self.tokenizer.src_lang = chunk['src_lang'][idx]\n",
    "                inputs = self.tokenizer(chunk['src_text'][idx], return_tensors='pt')\n",
    "                self.tokenizer.tgt_lang = chunk['tgt_lang'][idx]\n",
    "                with self.tokenizer.as_target_tokenizer():\n",
    "                    labels = self.tokenizer(chunk['tgt_text'][idx], return_tensors='pt').input_ids\n",
    "                tokenized_data['input_ids'] = inputs['input_ids'].reshape(-1)\n",
    "                tokenized_data['attention_mask'] = inputs['attention_mask'].reshape(-1)\n",
    "                tokenized_data['labels'] = labels.reshape(-1)\n",
    "                \n",
    "                length.append(len(tokenized_data['input_ids']))\n",
    "                length.append(len(tokenized_data['attention_mask']))\n",
    "                length.append(len(tokenized_data['labels']))\n",
    "                yield tokenized_data, length, worker_id\n",
    "                \n",
    "\n",
    "dataset = MyDataset('test2.json', tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8d763",
   "metadata": {},
   "source": [
    "## Customizing Collate Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    '''\n",
    "    sample :\n",
    "    [{'input_ids':tensor([]), 'attention_mask':tensor([]), 'labels':tensor([])}, ...]\n",
    "    \n",
    "    +) dict는 batch size만큼 있다\n",
    "    \n",
    "    lengths :\n",
    "    ([32, 32, 30], [27, 27, 24], [16, 16, 19], [56, 56, 62])\n",
    "    '''\n",
    "    \n",
    "    samples, lengths, _ = zip(*data)\n",
    "    ids_len, mask_len, label_len = zip(*lengths)\n",
    "    max_ids_len, max_mask_len, max_label_len = max(ids_len), max(mask_len), max(label_len)\n",
    "    ids_features = torch.zeros((len(data), max_ids_len), dtype=int)\n",
    "    mask_features = torch.zeros((len(data), max_mask_len), dtype=int)\n",
    "    label_features = torch.zeros((len(data), max_label_len), dtype=int)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        ids_idx = ids_len[i]\n",
    "        mask_idx = mask_len[i]\n",
    "        label_idx = label_len[i]\n",
    "        \n",
    "        ids_features[i] = torch.cat([samples[i]['input_ids'], torch.zeros((max_ids_len - ids_idx), dtype=int)])\n",
    "        mask_features[i] = torch.cat([samples[i]['attention_mask'], torch.zeros((max_mask_len - mask_idx), dtype=int)])\n",
    "        label_features[i] = torch.cat([samples[i]['labels'], torch.zeros((max_label_len - label_idx), dtype=int)])\n",
    "    batch = {'input_ids':ids_features, 'attention_mask':mask_features, 'labels':label_features}\n",
    "    return batch\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d169fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "model.to(device) # model-gpu\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * 10000\n",
    "# num_eval_steps = num_epochs * len(eval_dataloader)\n",
    "lr_scheduler = get_scheduler('linear',\n",
    "                            optimizer=optimizer,\n",
    "                            num_warmup_steps=0,\n",
    "                            num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab 에서는 돌아감\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "train_metric = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        batch = {k:v.to(device) for k, v in batch.items()} # data-gpu\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_loss /= num_training_steps\n",
    "    train_metric.append(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd72b8",
   "metadata": {},
   "source": [
    "## + High Level HF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "checkpoint = 'facebook/m2m100_418M'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(checkpoint, batched=True)\n",
    "\n",
    "def tokenize_function(data):\n",
    "    samples = []\n",
    "    tokenized_data = {'input_ids':None, 'attention_mask':None, 'labels':None}\n",
    "    for line in data:\n",
    "        tokenizer.src_lang = line['src_lang']\n",
    "        inputs = tokenizer(line['src_text'], return_tensors='pt')\n",
    "        tokenizer.tgt_lang = line['tgt_lang']\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(line['tgt_text'], return_tensors='pt').input_ids\n",
    "        tokenized_data['input_ids'] = inputs['input_ids'].reshape(-1)\n",
    "        tokenized_data['attention_mask'] = inputs['attention_mask'].reshape(-1)\n",
    "        tokenized_data['labels'] = labels.reshape(-1)\n",
    "        samples.append(tokenized_data)\n",
    "    return samples\n",
    "\n",
    "raw_dataset = Dataset.from_json('testing.json')\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
