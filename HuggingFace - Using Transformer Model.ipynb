{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631a059c",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- encoder model : 문장을 '이해'하는 데 탁월 + 분류/추출에 강세  \n",
    "- self-supervised : LM은 pretraining 훈련할 때 label 데이터 줄 필요 x 스스로 생성함  \n",
    "- decoder model : 문장을 '예측'하는 데 탁월 + auto-regressive 생성에 강세  \n",
    "- encoder-decoder model : 문장을 '생성'하는 데 탁월 (decoder와 특징이 거의 겹침)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c131fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# encoder model \n",
    "# classsification \n",
    "sent = pipeline('sentiment-analysis')\n",
    "sent('')\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "classifier('', candidate_labels=['A', 'B'])\n",
    "\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "ner('')\n",
    "\n",
    "\n",
    "# self-supervised\n",
    "# langauge model (encoder model)\n",
    "mask = pipeline('fill-mask')\n",
    "mask('[mask]', top_k=3)\n",
    "# language model (decoder model)\n",
    "gen = pipeline('text-generator')\n",
    "gen('', max_length=60, num_return_sequences=2)\n",
    "\n",
    "\n",
    "# encoder-decoder model\n",
    "qa = pipeline('question-answering')\n",
    "qa(question='', context='')\n",
    "\n",
    "summary = pipeline('summarization')\n",
    "summary('')\n",
    "\n",
    "trans = pipeline('translation', model='')\n",
    "trans('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9f4c3",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "```\n",
    "- 방법 1  : input => id  \n",
    "inputs = tokenizer(raw_inputs, padding=True, Truncation=True, return_tensors='pt')  \n",
    "\n",
    "- 방법 2  : input => token(subword) => id  \n",
    "tokens = tokenizer.tokenize(raw_inputs[0])  \n",
    "inputs = tokenizer.convert_tokens_to_ids(tokens)  \n",
    "```\n",
    "\n",
    "> 방법 1에는 SOS토큰과 EOS토큰이 추가된다  \n",
    "> 방법 1은 model(**inputs)를 해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64c5d4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 146, 1541, 1328, 102], [101, 1106, 1301, 1313, 106, 106, 106, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Pre-trained Tokenizer\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Pre-trained Bert Tokenizer\n",
    "bert_checkpoint = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_checkpoint)\n",
    "\n",
    "# Input (방법 1)\n",
    "# SOS토큰과 EOS토큰이 자동으로 추가된다\n",
    "raw_inputs = ['I really want', 'to go home!!!']\n",
    "inputs = tokenizer(raw_inputs)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136dbb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 : ['I', 'really', 'want']\n",
      "id : [146, 1541, 1328]\n"
     ]
    }
   ],
   "source": [
    "# Input (방법 2)\n",
    "sequence = raw_inputs[0] # 한 문장씩만 가능\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "inputs = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'토큰 : {tokens}\\nid : {inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id => text\n",
    "tokenizer.decode([146, 1541, 1328])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fd340",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('저장 위치')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9d6c0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea85045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# pre-trained Model \n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "output = model(**inputs) # 방법 1은 unpacking해야함\n",
    "output.last_hidden_state.shape\n",
    "\n",
    "# pre-trained Model 2 \n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "output = model(**inputs)\n",
    "predictions = F.softmax(output.logits, dim=-1) # Sentiment Analysis\n",
    "model.config.id2label\n",
    "\n",
    "# Bert Model 준비\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# pre-trained Bert Model\n",
    "model = BertModel.from_pretrained('bert_base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51938019",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('저장 위치')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a42e9",
   "metadata": {},
   "source": [
    "## Sum up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c112e3b",
   "metadata": {},
   "source": [
    "### 방법 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c18e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2031, 3524, 2000, 2131, 2125, 2013, 2147,  102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8568, -2.4364]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = 'I have wait to get off from work'\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors='pt')\n",
    "print(inputs['input_ids'])\n",
    "\n",
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f3ffc",
   "metadata": {},
   "source": [
    "### 방법 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34ef671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1045, 2031, 3524, 2000, 2131, 2125, 2013, 2147]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.9519, -2.4365]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = 'I have wait to get off from work'\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(input_ids) # <sos>와 <eos>가 없음\n",
    "\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f6a3d",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "- **padding id와 관계없이** attentoin mask를 해야 padding 처리한 부분을 참조하지 않는다\n",
    "\n",
    "attention mask로 padding token 부분을 가려줘야 한다  \n",
    "  \n",
    "그렇지 않으면 transformer 모델이 padding 토큰을 참조해서 '문맥'을 계산해버린다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c627b5",
   "metadata": {},
   "source": [
    "### 방법 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86c2fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['I am hesitating over saying something......', 'I want to get off from work']\n",
    "inputs = tokenizer(sequences, padding='longest')\n",
    "inputs = tokenizer(sequences, padding='max_length') # 모델에 적용할 수 있는 최대 seq len 까지 padding\n",
    "inputs = tokenizer(sequences, padding='max_length', max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902dd88",
   "metadata": {},
   "source": [
    "### 방법 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dca6f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895],\n",
       "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence1_id = [[200, 200, 200]]\n",
    "sequence2_id = [[200, 200]]\n",
    "batch_ids = [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]\n",
    "# tokenizer.pad_token_id == 0\n",
    "attention_mask = [[1,1,1],[1,1,0]]\n",
    "\n",
    "model(torch.tensor(batch_ids), attention_mask=torch.tensor(attention_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b71a7",
   "metadata": {},
   "source": [
    "## Truncate\n",
    "\n",
    "일반적으로 Transformer의 seq len은 512 또는 1024이다  \n",
    "  \n",
    "이보다 긴 경우 Truncate을 해야 한다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b0ac62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sequence = sequence[:max_sequence_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844d14a",
   "metadata": {},
   "source": [
    "### 방법 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85c85dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2572, 2002, 28032, 5844, 2058, 102], [101, 1045, 4299, 2000, 2131, 2125, 2013, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "sequences = ['I am hesitating over saying something......', 'I wish to get off from work']\n",
    "inputs = tokenizer(sequences, truncation=True) # 모델에 적용할 수 있는 최대 seq len를 넘으면 지운다\n",
    "inputs = tokenizer(sequences, max_length=8, truncation=True) # 8개를 넘으면 컷! <sos>, <eos> 포함 8개\n",
    "print(inputs)\n",
    "\n",
    "inputs = tokenizer(sequences, max_length=8, truncation=True, return_tensors='pt') # pytorch tensor\n",
    "inputs = tokenizer(sequences, max_length=8, truncation=True, return_tensors='tf') # tf tensor\n",
    "inputs = tokenizer(sequences, max_length=8, truncation=True, return_tensors='np') # numpy tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
