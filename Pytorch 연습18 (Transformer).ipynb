{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83071f39",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def tokenizer_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenizer_de,\n",
    "           init_token='<SOS>',\n",
    "           eos_token='<EOS>',\n",
    "           lower=True,\n",
    "           batch_first=True)\n",
    "TRG = Field(tokenize=tokenizer_en,\n",
    "           init_token='<SOS>',\n",
    "           eos_token='<EOS>',\n",
    "           lower=True,\n",
    "           batch_first=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                    fields=(SRC, TRG))\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                         batch_size=16,\n",
    "                                                         device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9590dc",
   "metadata": {},
   "source": [
    "## Encoder 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=100):\n",
    "        # input_dim, max_length : 인식범위\n",
    "        # hid dim == d_model\n",
    "        # pf_dim == d_ff\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim) # [len(src vocab), emb dim]\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim) # 우리 모델은 최대 max_length 만큼의 토큰 개수 만큼을 '한 문장'으로 받아들일 수 있다\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)]) \n",
    "        self.droput = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "    def forward(self, src, src_mask):\n",
    "        # src : [batch size, src time steps]\n",
    "        # src_mask : [batch size, 1, 1, src time steps]\n",
    "        batch_size = src.shape[0]\n",
    "        src_time_steps = src.shape[1]\n",
    "        \n",
    "        # batch마다 token index 구해서 token의 위치 값을 만든다\n",
    "        pos = torch.arange(0, src_time_steps).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        # pos : [batch size, src time steps]\n",
    "        \n",
    "        # 임베딩 테이블은 차원이 다르지만, 룩업 테이블은 차원이 같다. 그러므로 더할 수 있다.\n",
    "        src = self.dropout((self.tok_embedding * self.scale) + self.pos_embedding(pos))\n",
    "        # src : [batch size, src time steps, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src # Stacked Encoder의 최종 출력 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296f624",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6955a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward = PositionWiseFeedForward(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask):\n",
    "        # src : [batch size, src time steps, hid dim]\n",
    "        # src_mask : [batch size, 1, 1, src time steps]\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask) # QKV 계산하기 위해서 src 3개 copy\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src)) # Residual Connection + Norm\n",
    "        \n",
    "        _src = self.feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56414df",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ee21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query : [batch size, src time steps, hid dim]\n",
    "        # key : [batch size, src time steps, hid dim]\n",
    "        # value : [batch size, src time steps, hid dim]\n",
    "        batch_size = query.shape[0]\n",
    "        Q = self.fc_q(query) # [batch size, src time steps, hid dim]\n",
    "        K = self.fc_k(key) \n",
    "        V = self.fc_v(value) \n",
    "        \n",
    "        # [batch size, n_heads, src time stpes, head dim]\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale # Attention Score\n",
    "        # [batch size, n_heads, src time steps, src time steps]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0. -1e10) # padding(0인 부분)에 -무한대 넣기\n",
    "        \n",
    "        att_weights = torch.softmax(energy, dim=-1) # [batch size, n_heads, src time steps, src time steps]\n",
    "        \n",
    "        att_values = torch.matmul(self.dropout(att_weights), V)\n",
    "        att_values = att_values.permute(0, 2, 1, 3).contiguous()\n",
    "        # att_values : [batch size, src time steps, n heads, head dim]\n",
    "        att_values = att_values.view(batch_size, -1, self.hid_dim) # view로 concat을 대체한다\n",
    "        # att_values : [batch size, src time steps, hid dim]\n",
    "        att_values = att_values.fc_o(att_values)\n",
    "        \n",
    "        return att_values, att_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166078a4",
   "metadata": {},
   "source": [
    "## Position Wise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # x : [batch size, src time steps, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7da56",
   "metadata": {},
   "source": [
    "## Decoder 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg : [batch, trg time steps]\n",
    "        # enc_src : [batch, src time steps, hid dim]\n",
    "        # trg_mask : [batch, 1, trg time steps, trg time steps]\n",
    "        # src_mask : [batch, 1, 1, src time step]\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_time_steps = trg.shape[1]\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device) # [batch, trg time steps]\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)) # [batch, trg time steps, hid dim]\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # attention : [batch, n heads, trg time steps, src time steps]\n",
    "        output = self.fc_out(trg)\n",
    "        # output : [batch, trg time steps, len(trg vocab)]\n",
    "        return output, attention    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7b8c4",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a154bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward = PositionWiseFeedForward(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg       : [batch, trg time steps, hid dim]\n",
    "        # enc_src   : [batch, src time steps, hid dim]\n",
    "        # trg_mask  : [batch, 1, trg time steps, trg time steps]\n",
    "        # src_mask  : [batch, 1, 1, src time steps]\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee410a",
   "metadata": {},
   "source": [
    "## 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186797c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    def make_src_mask(self, src):\n",
    "        # src : [batch, src time steps]\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask : [batch, 1, 1, src time steps]\n",
    "        return src_mask\n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg : [batch, trg time steps]\n",
    "        \n",
    "        # 데이터의 pad만 True\n",
    "        # padding 전용 mask\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask : [batch, 1, 1, trg time step]\n",
    "        trg_time_steps = trg.shape[1]\n",
    "        \n",
    "        # diagonal 윗 부분만 False(=padding), 나머지는 True\n",
    "        # decoder 전용 mask\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_time_steps, trg_time_steps), device=self.device)).bool()\n",
    "        # trg_sub_mask : [trg time steps, trg time steps]\n",
    "        \n",
    "        # padding된 값인데(=False) decoder 마스크에는 포함이 안된다면? 최종적으로 False처리해야 함\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask # 둘 모두 참인 경우만 True로 살림\n",
    "        # trg mask : [batch, 1, trg time steps, trg time steps]\n",
    "        return trg_mask\n",
    "    def forward(self, src, trg):\n",
    "        # src : [batch ,src time steps, hid dim]\n",
    "        # trg : [batch, trg time steps, hid dim]\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        # output : [batch, trg time steps, trg vocab size]\n",
    "        # attention : [batch, n heads, trg time stpes, src time steps] = att weights\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a14f755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False],\n",
      "        [ True,  True]]) \n",
      " tensor([[[[ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True]]]]) \n",
      " tensor([[[[ True, False],\n",
      "          [ True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False],\n",
      "          [False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True, False],\n",
      "          [ True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.FloatTensor([[0, 0], [1, 1], [2, 2]])\n",
    "b = torch.FloatTensor([[3, 3], [1, 1], [4, 4]])\n",
    "\n",
    "sub = torch.tril(torch.ones((a.shape[1], a.shape[1]))).bool()\n",
    "pad = (a != b).unsqueeze(1).unsqueeze(2)\n",
    "print(sub, '\\n', pad, '\\n', sub & pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966f988",
   "metadata": {},
   "source": [
    "## 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM, OUTPUT_DIM = len(SRC.vocab), len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS, DEC_LAYERS = 3, 3\n",
    "ENC_HEADS, DEC_HEADS = 8, 8\n",
    "ENC_PF_DIM, DEC_PF_DIM = 512, 512\n",
    "ENC_DROPOUT, DEC_DROPOUT = 0.1, 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "src_pad_idx = SRC.vocab.stoi[SRC.pad_token]\n",
    "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2seq(enc, dec, src_pad_idx, trg_pad_idx, device).to(device)\n",
    "\n",
    "model.apply(lambda x: nn.init.xavier_uniform_(x.weight.data) if hasattr(x, 'weights') and x.weight.dim() > 1)\n",
    "\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94339788",
   "metadata": {},
   "source": [
    "## 훈련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch.src, batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1]) # <eos> 제외\n",
    "        # output : [batch, trg time steps - 1, len(trg vocab)]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim) # transpose ==> contiguous\n",
    "        trg = trg[:,1:].contiguous().view(-1) # <sos> 제외\n",
    "        # output : [batch * trg time steps - 1, output dim]\n",
    "        # trg : [batch * trg time steps -1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parmeters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7276d9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11],\n",
       "          [12, 13, 14, 15]],\n",
       " \n",
       "         [[16, 17, 18, 19],\n",
       "          [20, 21, 22, 23],\n",
       "          [24, 25, 26, 27],\n",
       "          [28, 29, 30, 31]]]),\n",
       " torch.Size([2, 4, 4]),\n",
       " (16, 4, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(0, 32).view(2,4,4)\n",
    "a, a.size(), a.stride()\n",
    "\n",
    "# stride는 자신의 axis 이전 값들을 모두 곱한 값이다\n",
    "# 16 = shape[1] * shape[2] = 4 * 4\n",
    "# 4 = shape[2] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7c09404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size:  torch.Size([2, 4, 4]) \n",
      " stride:  (16, 4, 1)\n",
      " transpose size:  torch.Size([4, 2, 4]) \n",
      " transpose stride:  (4, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "b = torch.randn(2,4,4)\n",
    "print(' size: ', b.size(), '\\n', 'stride: ', b.stride())\n",
    "print(' transpose size: ', b.transpose(0, 1).size(), '\\n','transpose stride: ', b.transpose(0, 1).stride())\n",
    "# transpose stride는 왜 8, 4, 1이 아닐까?\n",
    "# 그건 데이터를 읽는 방식 때문이다!\n",
    "\n",
    "# stride도 같이 transpose 해버린다!!!!\n",
    "# 즉, 데이터 자체를 바꾼게 아니라, 메모리에서 읽어들이는 인덱스만 바꾼 것이다.\n",
    "\n",
    "## 결론\n",
    "# Transpose를 썼으면 contiguous()를 써야한다\n",
    "# 그래야 데이터의 저장이 실제로 바뀌어서, 병렬 계산이 가능해진다\n",
    "# 즉, stride와 shape이 호환 가능해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770fe25",
   "metadata": {},
   "source": [
    "## 추론 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, trg = batch.src, batch.trg\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84499b05",
   "metadata": {},
   "source": [
    "## 에포크 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6066bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 1\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    print('train loss: ', train_loss, '\\n', 'valid loss: ', valid_loss)\n",
    "\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "print('test_loss: ', test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
