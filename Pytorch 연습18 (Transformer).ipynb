{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def tokenizer_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenizer_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenizer_de,\n",
    "           init_token='<SOS>',\n",
    "           eos_token='<EOS>',\n",
    "           lower=True,\n",
    "           batch_first=True)\n",
    "TRG = Field(tokenize=tokenizer_en,\n",
    "           init_token='<SOS>',\n",
    "           eos_token='<EOS>',\n",
    "           lower=True,\n",
    "           batch_first=True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n",
    "                                                    fields=(SRC, TRG))\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                         batch_size=16,\n",
    "                                                         device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=100):\n",
    "        # input_dim, max_length : 인식범위\n",
    "        # hid dim == d_model\n",
    "        # pf_dim == d_ff\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim) # [len(src vocab), emb dim]\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim) # 우리 모델은 최대 max_length 만큼의 토큰 개수 만큼을 '한 문장'으로 받아들일 수 있다\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)]) \n",
    "        self.droput = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "    def forward(self, src, src_mask):\n",
    "        # src : [batch size, src time steps]\n",
    "        # src_mask : [batch size, 1, 1, src time steps]\n",
    "        batch_size = src.shape[0]\n",
    "        src_time_steps = src.shape[1]\n",
    "        \n",
    "        # batch마다 token index 구해서 token의 위치 값을 만든다\n",
    "        pos = torch.arange(0, src_time_steps).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        # pos : [batch size, src time steps]\n",
    "        \n",
    "        # 임베딩 테이블은 차원이 다르지만, 룩업 테이블은 차원이 같다. 그러므로 더할 수 있다.\n",
    "        src = self.dropout((self.tok_embedding * self.scale) + self.pos_embedding(pos))\n",
    "        # src : [batch size, src time steps, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src # Stacked Encoder의 최종 출력 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6955a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward = PositionWiseFeedForward(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask):\n",
    "        # src : [batch size, src time steps, hid dim]\n",
    "        # src_mask : [batch size, 1, 1, src time steps]\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask) # QKV 계산하기 위해서 src 3개 copy\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src)) # Residual Connection + Norm\n",
    "        \n",
    "        _src = self.feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ee21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query : [batch size, src time steps, hid dim]\n",
    "        # key : [batch size, src time steps, hid dim]\n",
    "        # value : [batch size, src time steps, hid dim]\n",
    "        batch_size = query.shape[0]\n",
    "        Q = self.fc_q(query) # [batch size, src time steps, hid dim]\n",
    "        K = self.fc_k(key) \n",
    "        V = self.fc_v(value) \n",
    "        \n",
    "        # [batch size, n_heads, src time stpes, head dim]\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale # Attention Score\n",
    "        # [batch size, n_heads, src time steps, src time steps]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0. -1e10) # padding(0인 부분)에 -무한대 넣기\n",
    "        \n",
    "        att_weights = torch.softmax(energy, dim=-1) # [batch size, n_heads, src time steps, src time steps]\n",
    "        \n",
    "        att_values = torch.matmul(self.dropout(att_weights), V)\n",
    "        att_values = att_values.permute(0, 2, 1, 3).contiguous()\n",
    "        # att_values : [batch size, src time steps, n heads, head dim]\n",
    "        att_values = att_values.view(batch_size, -1, self.hid_dim) # view로 concat을 대체한다\n",
    "        # att_values : [batch size, src time steps, hid dim]\n",
    "        att_values = att_values.fc_o(att_values)\n",
    "        \n",
    "        return att_values, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # x : [batch size, src time steps, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
