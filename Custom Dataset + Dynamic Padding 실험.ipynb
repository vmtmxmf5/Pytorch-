{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89db791",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Tokenizer를 Class 안에서 encoding 함수를 만들어서 해결한다  \n",
    "\n",
    "getitem return을 {'input_ids':torch.tensor(), 'attention_mask':torch.tensor(), 'labels':torch.tensor()}  \n",
    "이 형식으로 만든다  \n",
    "  \n",
    "input_ids와 attention_mask의 torch tensor는 1차원이어야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aecd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53bb8bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7643, -0.3253,  0.3366]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb16e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.2275]]),\n",
       " tensor([[-1.0601, -1.9544]]),\n",
       " tensor([[-0.7470,  0.7065, -0.6278]]),\n",
       " tensor([[2.3750, 0.3567, 1.7337, 0.9199]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tmp = [torch.randn(1, lst) for lst in range(1, 5)]\n",
    "tmp2 = [torch.randn(1, lst) for lst in range(3, 7)]\n",
    "tmp3 = [np.random.randn(1, lst) for lst in range(1, 5)]\n",
    "tmp4 = [np.random.randn(1, lst) for lst in range(3, 7)]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a06bc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Tokenizer, DataCollatorWithPadding\n",
    "tokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_418M', batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffb08ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer):\n",
    "        super().__init__()\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "    def _encoding(self, key):\n",
    "        enc = []\n",
    "        for dic in self.samples:\n",
    "            token = self.tokenizer(dic[key], return_tensors='pt')\n",
    "            token['input_ids'] = token['input_ids'].reshape(-1)\n",
    "            token['attention_mask'] = token['attention_mask'].reshape(-1)\n",
    "#             token['token_type_ids'] = token['token_type_ids'].reshape(-1)\n",
    "            enc.append(token)\n",
    "        return enc\n",
    "#     def _labels(self, key):\n",
    "#         labels = []\n",
    "#         with self.tokenizer.as_target_tokenizer():\n",
    "#             labels = self.tokenizer().input_ids\n",
    "    def __getitem__(self, idx):\n",
    "#         return {'src_text':self._encoding('src_text')[idx]}, 'tgt_text':self._encoding('tgt_text')[idx]}\n",
    "        return self._encoding('src_text')[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dcce7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [{'src_text':'why so serious', 'tgt_text':'왤케 진지빰'},\n",
    "           {'src_text':'I am a boy', 'tgt_text':'난 소년헤헤'},\n",
    "           {'src_text':'my mind do it fire', 'tgt_text':'왈왈소리'}]\n",
    "\n",
    "dataset = MyDataset(samples, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e24c17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_text': {'input_ids': [128022, 203, 257, 8, 24091, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]},\n",
       " 'tgt_text': {'input_ids': [128022, 56888, 6864, 3639, 118323, 118323, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59f0954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_text': {'input_ids': [128022, 120764, 324, 123659, 2], 'attention_mask': [1, 1, 1, 1, 1]}, 'tgt_text': {'input_ids': [128022, 22, 3, 55792, 14258, 1452, 3, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}}\n",
      "{'src_text': {'input_ids': [128022, 203, 257, 8, 24091, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}, 'tgt_text': {'input_ids': [128022, 56888, 6864, 3639, 118323, 118323, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}}\n",
      "{'src_text': {'input_ids': [128022, 1949, 9963, 61, 862, 24923, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}, 'tgt_text': {'input_ids': [128022, 22, 126732, 126732, 3889, 1509, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}}\n"
     ]
    }
   ],
   "source": [
    "for i in iter(dataset):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd516e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([128022, 120764,    324, 123659,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([128022,    203,    257,      8,  24091,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([128022,   1949,   9963,     61,    862,  24923,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3984a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128022, 120764,    324, 123659,      2,      1,      1],\n",
      "        [128022,    203,    257,      8,  24091,      2,      1],\n",
      "        [128022,   1949,   9963,     61,    862,  24923,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3, collate_fn=data_collator)\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b492ee",
   "metadata": {},
   "source": [
    "### HF Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60b23e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jaehoon/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab543cf1ab08431aa2ed977e146d384b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jaehoon/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-079b17889f69c334.arrow\n",
      "Loading cached processed dataset at /home/jaehoon/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7c0047ab1a488ed1.arrow\n",
      "Loading cached processed dataset at /home/jaehoon/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2d555fabc6ea4aba.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b14fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), 'input_ids': tensor([  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
      "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
      "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
      "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
      "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]), 'labels': tensor(1), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1])}\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_datasets['train']:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8b16d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]]), 'input_ids': tensor([[  101,  1996,  2974,  1011, 17958, 17235,  2850,  4160, 12490,  5950,\n",
      "          1026,  1012, 11814,  2594,  1028, 26997,  2098,  2006,  1019,  1012,\n",
      "          6205,  2685,  1010,  2030,  1014,  1012,  2756,  3867,  1010,  2000,\n",
      "          1016,  1010,  5709,  2509,  1012,  2676,  1012,   102,  1996,  2974,\n",
      "          1011,  4208, 17235,  2850,  4160, 12490,  5950,  1026,  1012, 11814,\n",
      "          2594,  1028,  3935,  1020,  2685,  1010,  2030,  1014,  1012,  2382,\n",
      "          3867,  1010,  2000,  1016,  1010,  5709,  2509,  1010, 28500,  2075,\n",
      "          3041,  6409,  1012,   102,     0,     0,     0],\n",
      "        [  101,  2703,  4241, 13288, 17310,  2226,  1010,  2382,  1010,  2001,\n",
      "          5338,  2007,  4028,  1999,  2274, 13057, 22889,  4710,  8613,  2090,\n",
      "          2285,  1998,  2337,  1010,  1998,  2003,  2036,  6878,  1997,  1037,\n",
      "          2722,  4108,  4288,  1012,   102,  2703,  4241, 13288, 17310,  2226,\n",
      "          1010,  2382,  1010,  2001,  5338,  9857,  2007,  4028,  1999,  2274,\n",
      "         22889,  4710,  8613,  2090,  2285,  1998,  2337,  1010,  1998,  2036,\n",
      "          2003,  6878,  1997,  1037,  2722,  4288,  1999,  1996,  2110,  1997,\n",
      "          4108,  1012,   102,     0,     0,     0,     0],\n",
      "        [  101,  1996,  3438,  1011,  2095,  1011,  2214, 19965, 10308,  2010,\n",
      "         16214,  1010,  3038,  1024,  1000,  4067,  2017,  2061,  2172,  1012,\n",
      "          1000,   102,  5312,  2101,  1010,  2002, 10308,  2010,  3639,  9559,\n",
      "          1010,  5238,  3038,  1010,  1000,  4067,  2017,  1010,  2061,  2172,\n",
      "          1012,  1000,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2093,  2053,  4494,  2052,  3102,  2009,  2005,  2085,  1012,\n",
      "           102,  2009,  2052,  2202,  2093,  4494,  2000,  3102,  1996, 16222,\n",
      "          1005,  1055,  4935,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2019,  1999, 15500,  2046,  1996,  2482,  5823,  2008,  2730,\n",
      "          4615,  8805,  2097,  2022,  2218,  2254,  1020,  1010,  1996,  2548,\n",
      "          2155,  1005,  1055, 22896,  2623, 11585,  1012,   102,  1999, 15500,\n",
      "          2015,  2046,  1996,  6677,  1997,  8805,  1010,  4615,  1997,  3575,\n",
      "          1998, 26489,  2072, 19243,  2094,  2097,  2022,  6246,  2441,  1999,\n",
      "          1996,  2047,  2095,  1010,  1996,  2548,  2155,  1005,  1055, 22896,\n",
      "          2623,  7483,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9925,  3016,  3208, 10223,  9231,  9257,  2721,  1010,  6847,\n",
      "          2873,  2198, 18590,  1998,  2157, 25000, 18961, 21301,  2020, 22607,\n",
      "          2005,  9177,  2044, 21301,  2001,  2170,  2041,  2006,  9326,  2000,\n",
      "          2203,  1996,  6619,  1012,   102,  9925,  3016,  3208, 10223,  9231,\n",
      "          9257,  2721,  1010,  6847,  2873,  2198, 18590,  1998, 21301,  2035,\n",
      "          2020, 22607,  1999,  1996,  2690,  1997,  1996,  6619,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  5997,  2056,  2070,  1997,  9274,  1005,  1055,  8629,\n",
      "          2020, 15832,  1998,  2500,  2053,  2936,  2031,  1037,  2204,  3193,\n",
      "          1010,  2138,  1997,  6831,  3121,  2328,  2379,  2068,  1012,   102,\n",
      "          2500,  2053,  2936,  2031,  1037,  2204,  3193,  1010,  2009,  2056,\n",
      "          1010,  2138,  1997,  6831,  3121,  2328,  2379,  2068,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6819,  8159,  4305,  9251,  1996,  3820,  2018,  2025,  2042,\n",
      "          4844,  2011,  1996,  2604,  1998,  2008,  1996,  3477,  7245,  2001,\n",
      "         15884,  2445,  1996,  2110,  1997,  6819,  8159,  4305,  1005,  1055,\n",
      "         16156,  2043,  2047,  5766,  3744,  1011, 10731,  2176, 24826,  2165,\n",
      "          2058,  1012,   102,  6819,  8159,  4305,  9251,  1996,  3820,  2001,\n",
      "          2025,  4844,  2011,  2049,  2604,  1998,  2008,  1996,  3477,  7245,\n",
      "          2001, 15884,  2445,  1996,  2110,  1997,  6819,  8159,  4305,  1005,\n",
      "          1055, 16156,  2012,  1996,  2051,  1012,   102]]), 'labels': tensor([0, 1, 1, 1, 1, 0, 0, 1]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26ec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
